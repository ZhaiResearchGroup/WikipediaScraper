{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'wiki_data/'\n",
    "out_csv_path = 'documents.csv'\n",
    "urls_path = 'train_urls.txt'\n",
    "titles_path = '../wikipedia-biography-dataset/wikipedia-biography-dataset/train/train.title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document(document_lines):\n",
    "    start = 0\n",
    "    while start < len(document_lines) and document_lines[start] != \"Contents\\n\":\n",
    "        start += 1\n",
    "\n",
    "    doc = []\n",
    "    for i in range(start, len(document_lines)):\n",
    "        if document_lines[i][0] == \"^\":\n",
    "            continue\n",
    "\n",
    "        line_length = len(document_lines[i].split(\" \"))\n",
    "        if \"References\" == document_lines[i][:len(\"References\")] and line_length < 2:\n",
    "            break\n",
    "\n",
    "        if line_length > 10:\n",
    "            doc.append(document_lines[i])\n",
    "\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents_index(article_lines):\n",
    "    for i in range(len(article_lines)):\n",
    "        if article_lines[i] == 'Contents\\n':\n",
    "            return i\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def parse_summary(article_lines):\n",
    "    index = get_contents_index(article_lines)\n",
    "    \n",
    "    while(len(article_lines[index].split(\" \"))) < 8 and index >= 0:\n",
    "        index -= 1\n",
    "    \n",
    "    summary_indices = []\n",
    "    summary_indices.append(index)\n",
    "    \n",
    "    summary = []\n",
    "    while len(article_lines[index].split(\" \")) > 8 and index >= 0:\n",
    "        summary.insert(0, article_lines[index].strip())\n",
    "        index -= 1\n",
    "        \n",
    "    summary_indices.append(index)\n",
    "        \n",
    "    return ' '.join(summary), summary_indices[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS_TO_IGNORE = [\"See also\", \"References\", \"Bibliography\", \"External links\"]\n",
    "\n",
    "def parse_content_headers(article_lines):\n",
    "    index = get_contents_index(article_lines) + 1\n",
    "    \n",
    "    headers = []\n",
    "    \n",
    "    while True:\n",
    "        line = article_lines[index]\n",
    "        if line[0].isnumeric():\n",
    "            header = \" \".join(line.split(\" \")[1:]).strip()\n",
    "            if header not in HEADERS_TO_IGNORE:\n",
    "                headers.append(header)\n",
    "        elif line[0] != \"\\n\":\n",
    "            break\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sidebar_index(article_lines, file):\n",
    "    title = file.strip('.txt')\n",
    "    for i in range(len(article_lines)):\n",
    "        if article_lines[i].lower().strip() == (title.strip()):\n",
    "            return i\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def parse_right_sidebar(article_lines, file, end_index):\n",
    "    index = get_sidebar_index(article_lines, file)\n",
    "    if index == -1:\n",
    "        return []\n",
    "    \n",
    "    sidebar = []\n",
    "    \n",
    "    for i in range(index, end_index):\n",
    "        line = article_lines[i]\n",
    "        if line != \"\\n\" and len(line.split(\" \")) <= 3:\n",
    "            sidebar.append(line.strip())\n",
    "    \n",
    "    return sidebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset_for_documents(csv_path, data_dir, urls_path, titles_path, num_docs):\n",
    "    df = pd.DataFrame(columns=[\n",
    "        'title', 'document number', 'document', 'summary', 'header', 'sidebar', 'url'\n",
    "    ])\n",
    "    \n",
    "    docs = []\n",
    "    summaries = []\n",
    "    numbers = []\n",
    "    used_document_count = 0\n",
    "    skipped_document_count = 0\n",
    "    ordered_urls = []\n",
    "    ordered_titles = []\n",
    "    wiki_headers = []\n",
    "    wiki_sidebars = []\n",
    "    \n",
    "    urls = open(urls_path, 'r').readlines()\n",
    "    titles = open(titles_path, 'r').readlines()\n",
    "    filenames = os.listdir(data_dir)\n",
    "    num_docs = num_docs if num_docs > 0 else len(filenames)\n",
    "\n",
    "    print(num_docs)\n",
    "    for i in range(num_docs):\n",
    "        filename = filenames[i]\n",
    "        doc_num = int(filename.split(\".\")[0])\n",
    "        extension = filename.split(\".\")[1]\n",
    "        if extension != 'txt':\n",
    "            continue\n",
    "            \n",
    "        title = titles[doc_num].replace('-lrb- ', '(').replace(' -rrb-', ')')\n",
    "        lines = open(data_dir + filename).readlines()\n",
    "        document = parse_document(lines)\n",
    "        summary, summary_indices = parse_summary(lines)\n",
    "        lower_bound, upper_bound = summary_indices\n",
    "        \n",
    "        headers = parse_content_headers(lines)\n",
    "        sidebar = parse_right_sidebar(lines, title, lower_bound)\n",
    "        \n",
    "        if document != \"\" and summary != \"\":\n",
    "            url = urls[doc_num]\n",
    "            \n",
    "            docs.append(document)\n",
    "            summaries.append(summary)\n",
    "            numbers.append(doc_num)\n",
    "            ordered_titles.append(title[:len(title) - 1])\n",
    "            ordered_urls.append(url[:len(url) - 1])\n",
    "            wiki_headers.append(' --- '.join(headers))\n",
    "            wiki_sidebars.append(' --- '.join(sidebar))\n",
    "            \n",
    "            used_document_count += 1\n",
    "        else:\n",
    "            skipped_document_count += 1\n",
    "                    \n",
    "        if used_document_count % 10000 == 0:\n",
    "            print(str(used_document_count) + ' used.')\n",
    "\n",
    "    df['document'] = docs\n",
    "    df['summary'] = summaries\n",
    "    df['document number'] = numbers\n",
    "    df['title'] = ordered_titles\n",
    "    df['header'] = wiki_headers\n",
    "    df['sidebar'] = wiki_sidebars\n",
    "    df['url'] = ordered_urls\n",
    "    \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    return used_document_count, skipped_document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "0 used.\n",
      "10000 used.\n",
      "20000 used.\n",
      "20000 used.\n",
      "30000 used.\n",
      "30000 used.\n",
      "30000 used.\n",
      "30000 used.\n",
      "30000 used.\n",
      "40000 used.\n",
      "50000 used.\n",
      "50000 used.\n",
      "50000 used.\n",
      "50000 used.\n",
      "50000 used.\n",
      "50000 used.\n",
      "50000 used.\n",
      "50000 used.\n",
      "60000 used.\n",
      "70000 used.\n",
      "70000 used.\n",
      "80000 used.\n",
      "90000 used.\n",
      "90000 used.\n",
      "100000 used.\n"
     ]
    }
   ],
   "source": [
    "used_count, skipped_count = parse_dataset_for_documents(out_csv_path, data_dir, urls_path, titles_path, 180000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(out_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31184"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df['sidebar']))\n",
    "df['sidebar'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
